{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDPP Course lab 2 (Resilient Distributed Datasets)\n",
    "\n",
    "<img src='files/lab2_header.png'></img>\n",
    "\n",
    "Welcome to the second lab of BDPP course! \n",
    "\n",
    "Here is a summary of what we will cover in this lab:\n",
    "\n",
    "- Work with the SparkContext and SparkSession objects\n",
    "- Speed benchmarking (local and on the cloud)\n",
    "- Work with Resilient Distributed Datasets\n",
    "- Implementing PageRank Algorithm using spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sparkcontext\"></a>\n",
    "## 2. Work with the SparkContext and SparkSession objects\n",
    "\n",
    "The Spark driver application uses the SparkContext object to allow a programming interface to interact with the driver application. The SparkContext object tells Spark how and where to access a cluster. The Google Cloud predefines the Spark context for you. The object name to access the Spark session is `sc`.\n",
    "\n",
    "In other environments, you need to pick an interpreter (for example, pyspark for Python) and create a SparkConf object to initialize a SparkContext object. For example:\n",
    "<br>\n",
    "`from pyspark import SparkContext, SparkConf`<br>\n",
    "`conf = SparkConf().setAppName(appName).setMaster(master)`<br>\n",
    "`sc = SparkContext(conf=conf)`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sparkcontext1\"></a>\n",
    "### 2.1 Invoke the SparkContext\n",
    "Run the following cell to invoke the SparkContext. \n",
    "\n",
    "__Note:__ If you ran this jupyter file using `pyspark` command or on the Google cloud, the SparkContext object might already exist in the memory. You can check this by running `sc`command in a notebook cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.12\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T01:55:06.088021Z",
     "start_time": "2023-04-16T01:55:02.317425Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"MyApp\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remember__ that you can create only one instance of the `SparkContext` object (`sc`) in each pyspark session. This means that if you run the above code snippet again, you get the following error:\n",
    "\n",
    "<font color='red'>ValueError</font>:`Cannot run multiple SparkContexts at once; existing SparkContext(app=My app, master=local) created by __init__ ... ` \n",
    "\n",
    "Moreover, you have to close your Spark session at the end of your program. This can be done by calling the `stop` function: `sc.stop()`. It ensures that you will not face any problem connecting to Spark again.\n",
    "\n",
    "The same goes for the `SparkSession` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Spark version\n",
    "Check the version of the Spark driver application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T01:55:08.577243Z",
     "start_time": "2023-04-16T01:55:08.520963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 SparkSession\n",
    "\n",
    "Prior to Spark 2.0.0, `sparkContext` was used as a channel to access all spark functionality. From Spark 2.0.0 onwards, `SparkSession` provides a single point of entry to interact with underlying Spark functionality and allows programming Spark with DataFrame and Dataset APIs. The Google Cloud predefines the Spark session for you. The object name to access Spark session is `spark`.\n",
    "\n",
    "In other environments, you need to pick an interpreter (for example, pyspark for Python) and create a Spark session object. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T01:55:10.275133Z",
     "start_time": "2023-04-16T01:55:10.146493Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Spark Lab2\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "except:\n",
    "    Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T01:55:14.049665Z",
     "start_time": "2023-04-16T01:55:12.765523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-8T2B7Q15.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x25798bceb50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the functionality available with sparkContext are also available in sparkSession. If you need to access `SparkContext` through SparkSession use `sparkContext` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T01:55:17.682270Z",
     "start_time": "2023-04-16T01:55:17.655832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-8T2B7Q15.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=MyApp>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Speed benchmark (Spark VS Pandas)\n",
    "\n",
    "Now, let's use the `sparkSession` object to run a simple benchmark by comparing reading a relatively big CSV file with pandas VS Spark. Although we are still running Spark on our local computer, it ends up reading the file faster than pandas (note that the result highly depends on the parallelization capabilities of your CPU). This demonstrates how Spark dataframes are much faster when compared to their pandas equivalent.\n",
    "\n",
    "For this experiment, we use a somewhat large Vermont vendor dataset. This data is accessible through [this link](https://data.vermont.gov/Finance/Vermont-Vendor-Payments/786x-sbp3). On this link, please select export and then choose CSV format. Download the file rename it to `Vermont_Vendor_Payments.csv` and place it in the `files` folder next to this notebook. Now, run the following two code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T01:55:25.476876Z",
     "start_time": "2023-04-16T01:55:21.033764Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381 ms ± 109 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# loading csv file with Spark\n",
    "housing = spark.read.csv(\"files/Vermont_Vendor_Payments.csv\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T01:55:25.737801Z",
     "start_time": "2023-04-16T01:55:25.475884Z"
    }
   },
   "outputs": [],
   "source": [
    "# installl pandas library if you don't have it.\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T01:55:57.550555Z",
     "start_time": "2023-04-16T01:55:28.751607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.42 s ± 442 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# loading csv file with Pandas\n",
    "df_pandas = pd.read_csv(\"files/Vermont_Vendor_Payments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "__Question1__: Use the cell below to report your observations from this experiment and compare the running times.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nSpark dataframes read the dataset in 427 milliseconds \\nPandas dataframes took 4.24 seconds\\n==> Spark dataframes are much faster  \\nconsedring that the result may depend on the CPU capabilities. \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\" \n",
    "Spark dataframes read the dataset in 427 milliseconds \n",
    "Pandas dataframes took 4.24 seconds\n",
    "==> Spark dataframes are much faster  \n",
    "consedring that the result may depend on the CPU capabilities. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Speed benchmark (π calculation)\n",
    "\n",
    "Spark can also be used for compute-intensive tasks. This code estimates π by \"throwing darts\" at a circle. We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. The fraction should be π / 4, so we use this to get our estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T01:55:59.912226Z",
     "start_time": "2023-04-16T01:55:59.903689Z"
    }
   },
   "outputs": [],
   "source": [
    "# π calculation code\n",
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "import random\n",
    "\n",
    "num_samples = 10000000 # you can change this number, e.g. try 1000000\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "def spark_pi_calc():\n",
    "    # here we do the pi calcaulation using Spark\n",
    "    count = spark.sparkContext.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "    return (4.0 * count / num_samples)\n",
    "\n",
    "def python_pi_calc():\n",
    "    # here we do the same calculation with python list comprehension\n",
    "    count = sum([inside(_) for throw in range(num_samples)])\n",
    "    return (4.0 * count / num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Spark] Pi is roughly: 3.1418092\n"
     ]
    }
   ],
   "source": [
    "# Depending on your hardware, this can take some time to fininsh.\n",
    "# You can reduce num_samples if it is taking too much time.\n",
    "\n",
    "print(\"[Spark] Pi is roughly:\", spark_pi_calc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T01:51:48.186757Z",
     "start_time": "2023-04-16T01:51:45.891214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Python] Pi is roughly: 3.1408568\n"
     ]
    }
   ],
   "source": [
    "# Depending on your hardware, this can take some time to fininsh.\n",
    "# You can reduce num_samples if it is taking too much time.\n",
    "\n",
    "print(\"[Python] Pi is roughly:\", python_pi_calc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use π calculation code to benchmark spark VS python. For small problems, python might work faster than spark because of the initial setup cost of spark. However, as the problem gets bigger, the spark code starts to show its benefit and runs faster than python.\n",
    "\n",
    "In the code below, we start from a small `num_samples` and keep doubling it until the python loop exceeds `max_time` (here it is set to 3 seconds by default). We collect running times for spark and python codes and produce a plot displaying time VS num_samples. \n",
    "\n",
    "We want you to play with the `max_time` parameter until the problem gets big enough so that you observe the spark code runs faster than the python code. This, of course, highly depends on the parallelization capacity of your CPU, and you may end up getting different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment. This may take a few minutes to run.\n",
      "You can change max_time value to increase or decrease run time.\n",
      "(please wait)\n",
      "0 / 2000\n",
      "3 / 2000\n",
      "31 / 2000\n",
      "74 / 2000\n",
      "164 / 2000\n",
      "312 / 2000\n",
      "666 / 2000\n",
      "1257 / 2000\n",
      "2000 / 2000\n",
      "Done! Total time = 413.68s\n"
     ]
    }
   ],
   "source": [
    "import timeit, time\n",
    "\n",
    "max_time = 20 # you can also try 1, 2, 5, and 10 depending on your hardware performance.\n",
    "\n",
    "print('Running experiment. This may take a few minutes to run.')\n",
    "print('You can change max_time value to increase or decrease run time.')\n",
    "print('(please wait)')\n",
    "\n",
    "num_samples = 10000\n",
    "steps = []\n",
    "python_times = []\n",
    "sparks_times = []\n",
    "\n",
    "def my_timeit(func):\n",
    "    runs = 3  # If the experiment is still taking too much time to run, you may decrease this value as well.\n",
    "    dtime = timeit.timeit(func, number=runs)\n",
    "    elapsed = dtime/runs\n",
    "    return elapsed\n",
    "\n",
    "start = time.time()\n",
    "while True:\n",
    "    pt = my_timeit(python_pi_calc)\n",
    "    st = my_timeit(spark_pi_calc)\n",
    "    python_times.append(pt)\n",
    "    sparks_times.append(st)\n",
    "    steps.append(num_samples)\n",
    "    print(min(int(pt * 100), max_time*100), '/', max_time*100)\n",
    "    if pt > max_time:\n",
    "        break\n",
    "        \n",
    "    if pt > max_time:\n",
    "        break\n",
    "    elif pt < 0.1:\n",
    "        num_samples = num_samples * 10\n",
    "    else:\n",
    "        num_samples = num_samples * 2\n",
    "print(f\"Done! Total time = {time.time()-start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(steps, python_times, color='red', label='python')\n",
    "plt.plot(steps, sparks_times, color='blue', label='spark')\n",
    "plt.legend()\n",
    "plt.xlabel('number of samples', fontsize=12)\n",
    "plt.ylabel('running time (seconds)', fontsize=12)\n",
    "plt.title('Speed benchmark (π calculation)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "__Question2__: Use the cell below to report your observations from this experiment and compare the running times. Can Spark implementation run faster than Pandas on a single computer? How?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 0 / 500\\n2 / 500\\n32 / 500\\n64 / 500\\n132 / 500\\n327 / 500\\n500 / 500\\nDone! Total time = 99.26s\\n\\n0 / 1000\\n3 / 1000\\n33 / 1000\\n65 / 1000\\n128 / 1000\\n327 / 1000\\n569 / 1000\\n1000 / 1000\\nDone! Total time = 191.40s\\n\\n0 / 2000\\n3 / 2000\\n32 / 2000\\n66 / 2000\\n129 / 2000\\n302 / 2000\\n547 / 2000\\n1153 / 2000\\n2000 / 2000\\nDone! Total time = 367.25s\\n\\n\\nas a result of this experiment, Spark can run faster on a single computer due to its parallel processing capabilities. \\nWhen the problem size increases, Spark can distribute the workload across multiple CPU cores\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 0 / 500\n",
    "2 / 500\n",
    "32 / 500\n",
    "64 / 500\n",
    "132 / 500\n",
    "327 / 500\n",
    "500 / 500\n",
    "Done! Total time = 99.26s\n",
    "\n",
    "0 / 1000\n",
    "3 / 1000\n",
    "33 / 1000\n",
    "65 / 1000\n",
    "128 / 1000\n",
    "327 / 1000\n",
    "569 / 1000\n",
    "1000 / 1000\n",
    "Done! Total time = 191.40s\n",
    "\n",
    "0 / 2000\n",
    "3 / 2000\n",
    "32 / 2000\n",
    "66 / 2000\n",
    "129 / 2000\n",
    "302 / 2000\n",
    "547 / 2000\n",
    "1153 / 2000\n",
    "2000 / 2000\n",
    "Done! Total time = 367.25s\n",
    "\n",
    "\n",
    "as a result of this experiment, Spark can run faster on a single computer due to its parallel processing capabilities. \n",
    "When the problem size increases, Spark can distribute the workload across multiple CPU cores\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hint__: If you pick a big enough value for the `num_samples` parameter (code below), you should be able to see multiple python processes running at the same time in your (system monitor/task manager) when benchmarking spark code (A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1416584933333334"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code (A) Spark \n",
    "# Depending on your hardware, this can take some time to fininsh.\n",
    "# You can reduce num_samples if it is taking too much time.\n",
    "\n",
    "num_samples = 300000000  # reduce this number if it is taking too much time to run\n",
    "spark_pi_calc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1418972"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code (B) Python - List Comprehension\n",
    "# Depending on your hardware, this can take some time to fininsh.\n",
    "# You can reduce num_samples if it is taking too much time.\n",
    "\n",
    "num_samples = 100000000  # reduce this number if it is taking too much time to run\n",
    "python_pi_calc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<img src=\"files/spark run.png\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "This picture also demonstrates how Spark uses CPU VS python:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"files/spark cpu load.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Speed benchmark on the Cloud (recommender system)\n",
    "\n",
    "In this section, you are required to run a recommender system written using spark Mlib (cell below) with (at least) the following two configurations and compare their speed in a brief report:\n",
    "\n",
    "1. Single Node (1 master (__2 vCPU__), __ZERO__ workers)\n",
    "1. Standard (1 master (__1 vCPU__), __SEVEN__ workers (__1 vCPU each__))\n",
    "\n",
    "__Change__ the machine type for both configurations to `Custom` and set the number of __vCPU__ core for master and worker nodes according to the specified configuration above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Because of the resource limitations, you may not be able to create both cluster types together. If this issue happens, you can delete the previous cluster before making a new one.\n",
    "Following are a few tips on how to create different types of clusters in GCP:\n",
    "\n",
    "- This is how you can change the cluster type:\n",
    "<img src=\"files/benchmark0.png\">\n",
    "- This is how you can change `number of workers` and `Machine Types`:\n",
    "<img src=\"files/benchmark1.png\">\n",
    "- For the second experiment set `number of workers` to 7 in `standard cluster`:\n",
    "<img src=\"files/benchmark2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two files `u.data` and `u.item` located in the `files` folder. You should upload these files to your cloud storage and change the `your_bucket_name` variable according to your bucket name.\n",
    "\n",
    "Here is the benchmark code you should run on the cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Blob: myproject-bdpp2023-bucket1, Vermont_Vendor_Payments.csv, 1681577358439728>\n",
      "<Blob: myproject-bdpp2023-bucket1, housing.csv, 1679697296954570>\n",
      "<Blob: myproject-bdpp2023-bucket1, u.data, 1681605790906197>\n",
      "<Blob: myproject-bdpp2023-bucket1, u.item, 1681605790780514>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/16 01:16:15 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/04/16 01:16:15 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/04/16 01:16:15 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/04/16 01:16:15 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading movie names...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/16 01:16:25 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n",
      "java.lang.InterruptedException\n",
      "\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n",
      "\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n",
      "\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n",
      "\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/16 01:16:27 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n",
      "java.lang.InterruptedException\n",
      "\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n",
      "\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n",
      "\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n",
      "\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training recommendation model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/16 01:16:38 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/04/16 01:16:38 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "23/04/16 01:16:39 WARN com.github.fommil.netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "23/04/16 01:16:39 WARN com.github.fommil.netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163.43715167045593\n",
      "\n",
      "Ratings for user ID 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rosewood (1997): 4.0\n",
      "Shall We Dance? (1996): 5.0\n",
      "Star Wars (1977): 5.0\n",
      "3 Ninjas: High Noon At Mega Mountain (1998): 1.0\n",
      "Ulee's Gold (1997): 4.0\n",
      "Fierce Creatures (1997): 3.0\n",
      "Midnight in the Garden of Good and Evil (1997): 3.0\n",
      "River Wild, The (1994): 3.0\n",
      "Mighty Aphrodite (1995): 4.0\n",
      "Up Close and Personal (1996): 3.0\n",
      "Ulee's Gold (1997): 4.0\n",
      "FairyTale: A True Story (1997): 3.0\n",
      "Devil's Advocate, The (1997): 3.0\n",
      "Men in Black (1997): 4.0\n",
      "As Good As It Gets (1997): 5.0\n",
      "Apt Pupil (1998): 1.0\n",
      "In & Out (1997): 4.0\n",
      "Titanic (1997): 5.0\n",
      "Once Upon a Time... When We Were Colored (1995): 4.0\n",
      "Hoodlum (1997): 4.0\n",
      "Face/Off (1997): 3.0\n",
      "Antonia's Line (1995): 3.0\n",
      "Restoration (1995): 4.0\n",
      "Time to Kill, A (1996): 4.0\n",
      "Truth About Cats & Dogs, The (1996): 4.0\n",
      "Contact (1997): 3.0\n",
      "Breakdown (1997): 4.0\n",
      "Kolya (1996): 5.0\n",
      "Emma (1996): 5.0\n",
      "Leaving Las Vegas (1995): 4.0\n",
      "Toy Story (1995): 4.0\n",
      "Ice Storm, The (1997): 3.0\n",
      "Postino, Il (1994): 4.0\n",
      "Marvin's Room (1996): 3.0\n",
      "Absolute Power (1997): 3.0\n",
      "Donnie Brasco (1997): 4.0\n",
      "Liar Liar (1997): 1.0\n",
      "Rainmaker, The (1997): 4.0\n",
      "Deceiver (1997): 1.0\n",
      "Mrs. Brown (Her Majesty, Mrs. Brown) (1997): 4.0\n",
      "Birdcage, The (1996): 4.0\n",
      "Heat (1995): 4.0\n",
      "Richard III (1995): 2.0\n",
      "Wings of the Dove, The (1997): 5.0\n",
      "Full Monty, The (1997): 4.0\n",
      "My Best Friend's Wedding (1997): 4.0\n",
      "Tin Cup (1996): 4.0\n",
      "Sabrina (1995): 3.0\n",
      "Jerry Maguire (1996): 4.0\n",
      "Air Force One (1997): 4.0\n",
      "Fargo (1996): 5.0\n",
      "Godfather, The (1972): 5.0\n",
      "Secrets & Lies (1996): 5.0\n",
      "Evita (1996): 3.0\n",
      "Fly Away Home (1996): 4.0\n",
      "Good Will Hunting (1997): 5.0\n",
      "Bed of Roses (1996): 3.0\n",
      "Scream (1996): 3.0\n",
      "English Patient, The (1996): 4.0\n",
      "Sense and Sensibility (1995): 5.0\n",
      "L.A. Confidential (1997): 5.0\n",
      "Promesse, La (1996): 3.0\n",
      "\n",
      "Top 10 recommendations:\n",
      "Angel Baby (1995) score 7.035786315950679\n",
      "Boys, Les (1997) score 6.465307980696137\n",
      "Chungking Express (1994) score 5.959988304202963\n",
      "Shall We Dance? (1937) score 5.820296292827166\n",
      "Mr. Smith Goes to Washington (1939) score 5.629080026180951\n",
      "Top Hat (1935) score 5.612098754133056\n",
      "Casablanca (1942) score 5.577760835635937\n",
      "SubUrbia (1997) score 5.567909749202793\n",
      "Kids (1995) score 5.5066554764272775\n",
      "Underneath, The (1995) score 5.433960598098182\n",
      "Done! Total time = 164.80s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.recommendation import ALS, Rating\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "your_bucket_name = 'myproject-bdpp2023-bucket1'\n",
    "\n",
    "client = storage.Client()\n",
    "for blob in client.list_blobs(your_bucket_name):\n",
    "    print(str(blob))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    item = sc.textFile(\"gs://\" + your_bucket_name + \"/u.item\")\n",
    "    for line in item.collect():\n",
    "        fields = line.split('|')\n",
    "        movieNames[int(fields[0])] = fields[1]\n",
    "\n",
    "    return movieNames\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MovieRecommendationsALS\")\n",
    "try:\n",
    "    sc = SparkContext(conf = conf)\n",
    "except:\n",
    "    pass\n",
    "sc.setCheckpointDir('checkpoint')\n",
    "\n",
    "print(\"\\nLoading movie names...\")\n",
    "nameDict = loadMovieNames()\n",
    "\n",
    "\n",
    "data = sc.textFile(\"gs://\" + your_bucket_name + \"/u.data\")\n",
    "\n",
    "ratings = data.map(lambda l: l.split()).map(lambda l: Rating(int(l[0]), \\\n",
    "                            int(l[1]), float(l[2])))#.cache()\n",
    "\n",
    "ratings.collect()\n",
    "\n",
    "data.collect()\n",
    "\n",
    "start = time.time()\n",
    "# Build the recommendation model using Alternating Least Squares\n",
    "print(\"\\nTraining recommendation model...\")\n",
    "rank = 10\n",
    "# Lowered numIterations to ensure it works on lower-end systems\n",
    "numIterations = 1000\n",
    "model = ALS.train(ratings, rank, numIterations)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "userID = 2\n",
    "\n",
    "print(\"\\nRatings for user ID \" + str(userID) + \":\")\n",
    "userRatings = ratings.filter(lambda l: l[0] == userID)\n",
    "for rating in userRatings.collect():\n",
    "    print (nameDict[int(rating[1])] + \": \" + str(rating[2]))\n",
    "\n",
    "print(\"\\nTop 10 recommendations:\")\n",
    "recommendations = model.recommendProducts(userID, 10)\n",
    "for recommendation in recommendations:\n",
    "    print (nameDict[int(recommendation[1])] + \\\n",
    "        \" score \" + str(recommendation[2]))\n",
    "\n",
    "print(f\"Done! Total time = {time.time()-start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "__Question3__: Run the above experiment three times for each configuration and report the running times and mean and standard deviation for each setup. Discuss how adding extra resources affects the running time?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFiguration 1:\\nExperiment 1: 155.41 seconds\\nExperiment 2: 156.85 seconds\\nExperiment 3: 154.24 seconds\\nMean: 155.83 seconds\\nstandard Deviation: 1.06 seconds\\n\\nFiguration 2:\\nExperiment 1: 163.43 seconds\\nExperiment 2: 163.73 seconds\\nExperiment 3: 162.55 seconds\\nMean: 163.24 seconds\\nStandard Deviation: 0.51 seconds\\n\\nas a result adding extra resources can speed up the execution of programs\\nIn this case, since the program is relatively simple, adding extra resources did not lead to a\\nsignificant reduction in running time.\\n\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Figuration 1:\n",
    "Experiment 1: 155.41 seconds\n",
    "Experiment 2: 156.85 seconds\n",
    "Experiment 3: 154.24 seconds\n",
    "Mean: 155.83 seconds\n",
    "standard Deviation: 1.06 seconds\n",
    "\n",
    "Figuration 2:\n",
    "Experiment 1: 163.43 seconds\n",
    "Experiment 2: 163.73 seconds\n",
    "Experiment 3: 162.55 seconds\n",
    "Mean: 163.24 seconds\n",
    "Standard Deviation: 0.51 seconds\n",
    "\n",
    "as a result ==> adding extra resources can speed up the programs' execution\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd\"></a>\n",
    "## 3. Work with Resilient Distributed Datasets\n",
    "Spark uses an abstraction for working with data called a Resilient Distributed Dataset (RDD). An RDD is a collection of elements that can be operated on in parallel. RDDs are immutable, so you can't update the data in them. To update data in an RDD, you must create a new RDD. In Spark, all work is done by creating new RDDs, transforming existing RDDs, or using RDDs to compute results. When working with RDDs, the Spark driver application automatically distributes the work across the cluster.\n",
    "\n",
    "You can construct RDDs by parallelizing existing Python collections (lists), by manipulating RDDs, or by manipulating files in HDFS or any other storage system.\n",
    "\n",
    "You can run these types of methods on RDDs: \n",
    " - Actions: query the data and return values\n",
    " - Transformations: manipulate data values and return pointers to new RDDs. \n",
    "\n",
    "Find more information on Python methods in the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html\" target=\"_blank\" rel=\"noopener noreferrer\">PySpark documentation</a>.\n",
    "\n",
    "<a id=\"rdd1\"></a>\n",
    "### 3.1 Create a collection\n",
    "Create a Python collection of the numbers 1 - 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd2\"></a>\n",
    "### 3.2 Create an RDD \n",
    "Put the collection into an RDD named `x_nbr_rdd` using the `parallelize` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nbr_rdd = spark.sparkContext.parallelize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there's no return value. The `parallelize` method didn't compute a result, which means it's a transformation. Spark just recorded how to create the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd3\"></a>\n",
    "### 3.3 View the data \n",
    "View the first element in the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nbr_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each number in the collection is in a different element in the RDD. Because the `first()` method returned a value, it is an action. \n",
    "\n",
    "Now view the first five elements in the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nbr_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd4\"></a>\n",
    "### 3.4 Create another RDD \n",
    "Create a Python collection that contains strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [\"Hello Human\", \"My Name is Spark\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the collection into an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_str_rdd = spark.sparkContext.parallelize(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the first element in the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Human']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_str_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You created the string \"Hello Human\" and you returned it as the first element of the RDD. To analyze a set of words, you can map each word into an RDD element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans\"></a>\n",
    "## 4. Manipulate data in RDDs\n",
    "\n",
    "Remember that to manipulate data, you use transformation functions.\n",
    "\n",
    "Here are some common Python transformation functions that you'll be using in this notebook:\n",
    "\n",
    " - `map(func)`: returns a new RDD with the results of running the specified function on each element  \n",
    " - `filter(func)`: returns a new RDD with the elements for which the specified function returns true   \n",
    " - `distinct([numTasks]))`: returns a new RDD that contains the distinct elements of the source RDD\n",
    " - `flatMap(func)`: returns a new RDD by first running the specified function on all elements, returning 0 or more results for each original element, and then flattening the results into individual elements\n",
    "\n",
    "You can also create functions that run a single expression and don't have a name with the Python `lambda` keyword. For example, this function returns the sum of its arguments: `lambda a , b : a + b`.\n",
    "\n",
    "<a id=\"trans1\"></a>\n",
    "### 4.1 Update numeric values\n",
    "Run the `map()` function with the `lambda` keyword to replace each element, X, in your first RDD (the one that has numeric values) with X+1. Because RDDs are immutable, you need to specify a new RDD name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO_4: Replace <FILL IN> with appropriate code\n",
    "\n",
    "x_nbr_rdd_2 = x_nbr_rdd.map(lambda x: x + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the elements of the new RDD: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nbr_rdd_2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with the `collect` method! It returns __all__ elements of the RDD to the driver. Returning a large data set might be not be very useful. No-one wants to scroll through a million rows!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans2\"></a>\n",
    "### 4.2 Add numbers in an array\n",
    "An array of values is a common data format where multiple values are contained in one element. You can manipulate the individual values if you split them up into separate elements.\n",
    "\n",
    "Create an array of numbers by including quotation marks around the whole set of numbers. If you omit the quotation marks, you get a collection of numbers instead of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\"1,2,3,4,5,6,7,8,9,10\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD for the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rd = spark.sparkContext.parallelize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the values at commas and add values in the positions 3 and 7 in the array. Keep in mind that an array starts with position 0. Use a backslash character, \\, to break the line of code for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO_5: Replace <FILL IN> with appropriate code\n",
    "\n",
    "Sum_rd = y_rd.map(lambda y: y.split(',')) \\\n",
    "             .map(lambda y: int(y[3]) + int(y[7]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now return the value of the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sum_rd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `12`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans3\"></a>\n",
    "### 4.3 Split and count text strings\n",
    "\n",
    "Create an RDD with a text string and show the first element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Human. I'm Spark and I love running analysis on data.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Words = [\"Hello Human. I'm Spark and I love running analysis on data.\"]\n",
    "words_rd = spark.sparkContext.parallelize(Words)\n",
    "words_rd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the string into separate lines at the space characters and look at the first element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Human.',\n",
       " \"I'm\",\n",
       " 'Spark',\n",
       " 'and',\n",
       " 'I',\n",
       " 'love',\n",
       " 'running',\n",
       " 'analysis',\n",
       " 'on',\n",
       " 'data.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO_6: Replace <FILL IN> with appropriate code\n",
    "\n",
    "Words_rd2 = words_rd.map(lambda line: line.split(\" \"))\n",
    "Words_rd2.first()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of elements in this RDD with the `count()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Words_rd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you already knew that there was only one element because you ran the `first()` method and it returned the whole string. Splitting the string into multiple lines did not create multiple elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split the string again, but this time with the `flatmap()` method, and look at the first three elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'Human.', \"I'm\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO_7: Replace <FILL IN> with appropriate code\n",
    "\n",
    "words_rd2 = words_rd.flatMap(lambda line: line.split())\n",
    "words_rd2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `11`.\n",
    "This time each word is separated into its own element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans4\"></a>\n",
    "### 4.4 Count words with a pair RDD\n",
    "A common way to count the number of instances of words in an RDD is to create a pair RDD. A pair RDD converts each word into a key-value pair: the word is the key and the number 1 is the value. Because the values are all 1, when you add the  values for a particular word, you get the number of instances of that word.\n",
    "\n",
    "Create an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First,Line'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = [\"First,Line\", \"Second,Line\", \"and,Third,Line\"]\n",
    "z_str_rdd = spark.sparkContext.parallelize(z)\n",
    "z_str_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the elements into individual words with the `flatmap()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First', 'Line', 'Second', 'Line', 'and', 'Third', 'Line']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split(\",\"))\n",
    "z_str_rdd_split_flatmap.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the elements into key-value pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('First', 1),\n",
       " ('Line', 1),\n",
       " ('Second', 1),\n",
       " ('Line', 1),\n",
       " ('and', 1),\n",
       " ('Third', 1),\n",
       " ('Line', 1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countWords = z_str_rdd_split_flatmap.map(lambda word: (word, 1))\n",
    "countWords.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now sum all the values by key to find the number of instances for each word: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('First', 1), ('Line', 3), ('Second', 1), ('and', 1), ('Third', 1)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "countWords2 = countWords.reduceByKey(add)\n",
    "countWords2.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the word `Line` has a count of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"filter\"></a>\n",
    "## 5. Filter data\n",
    "\n",
    "The filter command creates a new RDD from another RDD based on a filter criteria.\n",
    "The filter syntax is: \n",
    "\n",
    "`.filter(lambda line: \"Filter Criteria Value\" in line)`\n",
    "\n",
    "Hint: Use a simple python `print` command to add a string to your Spark results and to run multiple actions in single cell.\n",
    "\n",
    "Find the number of instances of the word `Line` in the `z_str_rdd_split_flatmap` RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of words Line\n",
      "Is: 3\n"
     ]
    }
   ],
   "source": [
    "# TODO_8: Replace <FILL IN> with appropriate code\n",
    "\n",
    "words_rd3 = z_str_rdd_split_flatmap.filter(lambda word: \"Line\" in word)\n",
    "\n",
    "print(\"The count of words \" + str(words_rd3.first()))\n",
    "print(\"Is: \" + str(words_rd3.count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile\"></a>\n",
    "## 6. Analyze text data from a file\n",
    "In this section, you'll use a text file `README.txt` to create an RDD from it, and analyze the text in it. The file should already exist on `files`folder next to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile2\"></a>\n",
    "### 6.2 Create an RDD from the file\n",
    "Use the `textFile` method to create an RDD named `textfile_rdd` based on the `README.txt` file. The RDD will contain one element for each line in the `README.txt` file.\n",
    "Also, count the number of lines in the RDD, which is the same as the number of lines in the text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textfile_rdd = spark.sparkContext.textFile(\"files/README.txt\")\n",
    "textfile_rdd.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile3\"></a>\n",
    "### 6.3 Filter for a word \n",
    "Filter the RDD to keep only the elements that contain the word \"Spark\" with the `filter` transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Apache Spark'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO_9: Replace <FILL IN> with appropriate code\n",
    "Spark_lines = textfile_rdd.filter(lambda line: \"Spark\" in line)\n",
    "Spark_lines.first()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `'# Apache Spark'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of elements in this filtered RDD and present the result as a concatenated string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file README.txt has 19 of 98 Lines with word Spark in it.\n"
     ]
    }
   ],
   "source": [
    "# TODO_10: Replace <FILL IN> with appropriate code\n",
    "\n",
    "print(\"The file README.txt has \" + str(Spark_lines.count()) + \\\n",
    "      \" of \" + str(textfile_rdd.count()) + \\\n",
    "      \" Lines with word Spark in it.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `The file README.txt has 19 of 98 Lines with word Spark in it.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile4\"></a>\n",
    "### 6.4 Count the instances of a string at the beginning of words\n",
    "Count the number of times the substring \"Spark\" appears at the beginning of a word in the original text.\n",
    "\n",
    "Here's what you need to do: \n",
    "\n",
    "1. Run a `flatMap` transformation on the Spark_lines RDD and split on white spaces.\n",
    "2. Create an RDD with key-value pairs where the first element of the tuple is the word and the second element is the number 1.\n",
    "3. Run a `reduceByKey` method with the `add` function to count the number of instances of each word.<br>\n",
    "4. Filter the resulting RDD to keep only the elements that start with the word \"Spark\". In Python, the syntax to determine whether a string starts with a token is: `string.startswith(\"token\")` \n",
    "5. Display the resulting list of elements that start with \"Spark\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 14),\n",
       " ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
       " ('SparkPi', 2),\n",
       " ('Spark](#building-spark).', 1),\n",
       " ('Spark.', 1)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO_11: write your code here.\n",
    "split_words = Spark_lines.flatMap(lambda line: line.split(\" \"))\n",
    "word_pairs = split_words.map(lambda word: (word, 1))\n",
    "word_counts = word_pairs.reduceByKey(add)\n",
    "filtered_words = word_counts.filter(lambda pair: pair[0].startswith(\"Spark\"))\n",
    "\n",
    "filtered_words.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see:<br>\n",
    "<pre>\n",
    "[('Spark', 14),\n",
    " ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
    " ('SparkPi', 2),\n",
    " ('Spark](#building-spark).', 1),\n",
    " ('Spark.', 1)]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile5\"></a>\n",
    "### 6.5 Count instances of a string within words\n",
    "Now filter and display the elements that contain the substring \"Spark\" anywhere in the word, instead of just at the beginning of words like the last section. Your result should be a superset of the previous result.\n",
    "\n",
    "The Python syntax to determine whether a string contains a particular token is: `\"token\" in string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 14),\n",
       " ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
       " ('SparkPi', 2),\n",
       " ('Spark](#building-spark).', 1),\n",
       " ('tests](https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-AutomatedTesting).',\n",
       "  1),\n",
       " ('Spark.', 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO_12: write your code here.\n",
    "filtered_words_anywhere = word_counts.filter(lambda pair: \"Spark\" in pair[0])\n",
    "\n",
    "filtered_words_anywhere.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see:\n",
    "<pre>\n",
    "[('Spark', 14),\n",
    " ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
    " ('SparkPi', 2),\n",
    " ('Spark](#building-spark).', 1),\n",
    " ('Spark.', 1),\n",
    " ('tests](https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-AutomatedTesting).',\n",
    "  1)]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"numfile\"></a>\n",
    "## 7. Analyze numeric data from a file\n",
    "You'll analyze a sample file `Scores.txt` given in `files` folder that contains instructor names and scores. The file has the following format: Instructor Name,Score1,Score2,Score3,Score4,... The number of scores for each instructor could be diferent.\n",
    "Here is an example line from the text file: \"Carlo,5.5,3,3,4\" or \"Pablo,9,10,8.6,7,9,5,6\"\n",
    "Your task is to look at all the scores from each instructor and find the maximum score given by each instructor:\n",
    "\n",
    "1. Load the text file into an RDD.\n",
    "1. Run a transformation to create an RDD with the instructor names and the scores per instructor.\n",
    "1. Run a second transformation to compute the maximum score for each instructor.\n",
    "1. Display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tobias', 8.0),\n",
       " ('Malin', 10.0),\n",
       " ('Ali', 8.7),\n",
       " ('Magnus', 5.0),\n",
       " ('Alice', 9.1),\n",
       " ('Jack', 7.4)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_rdd = spark.sparkContext.textFile(\"files/Scores.txt\")\n",
    "instructors_scores = scores_rdd.map(lambda line: line.split(\",\"))\n",
    "max_scores = instructors_scores.map(lambda x: (x[0], max(map(float, x[1:]))))\n",
    "max_scores.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see:\n",
    "<pre>\n",
    "[('Tobias', 8.0),\n",
    " ('Malin', 10.0),\n",
    " ('Ali', 8.7),\n",
    " ('Magnus', 5.0),\n",
    " ('Alice', 9.1),\n",
    " ('Jack', 7.4)]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. PageRank Algorithm\n",
    "\n",
    "In the final task, we are interested in using Spark to rank a list of websites based on their importance. One obvious application of such an analysis is to provide the ordering for web searches. To measure the importance of a page, you are tasked to calculate `PageRank`. PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites. The PageRank algorithm is very well described in Chapter 5 of the book `Mining Massive Data Sets`. You can access the book through [this link](http://www.mmds.org/).\n",
    "\n",
    "The websites are stored in a file named `urls.txt` located in the `files` folder. The input file has the following format:\n",
    "\n",
    "<pre>\n",
    "URL, neighbor URL\n",
    "URL, neighbor URL\n",
    "URL, neighbor URL\n",
    "...\n",
    "</pre>\n",
    "\n",
    "\n",
    "In the output, sort the websites descending based on their level of importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from operator import add\n",
    "\n",
    "def compute_contributions(urls, rank):\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "url_file = \"files/urls_link.txt\"\n",
    "iterations = 10\n",
    "\n",
    "lines = spark.sparkContext.textFile(url_file)\n",
    "links = lines.map(lambda line: re.split(r'\\s+', line)).groupByKey()\n",
    "ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
    "\n",
    "for _ in range(iterations):\n",
    "    contributions = links.join(ranks).flatMap(\n",
    "        lambda url_urls_rank: compute_contributions(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "    ranks = contributions.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "\n",
    "sorted_ranks = ranks.sortBy(lambda x: x[1], ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PageRank` algorithm has some variants. Based on how you implement the algorithm and the parameters you choose, you will end up geting different scores and ranks. Following is one example output. Make sure your produced ranking makes sense, e.g. `google.com` should recieve higher score compared to `merriam-webster.com`.\n",
    "\n",
    "<pre>\n",
    "1) google.com PageRank:1.8086487117025336\n",
    "2) wikipedia.org PageRank:1.8086487117025336\n",
    "3) facebook.com PageRank:1.777648228912891\n",
    "4) youtube.com PageRank:1.7021245199883723\n",
    "5) twitter.com PageRank:1.5510244219266531\n",
    "6) amazon.com PageRank:1.3904212544003804\n",
    "7) imdb.com PageRank:1.3008511398069642\n",
    "8) merriam-webster.com PageRank:1.1146111418335307\n",
    "9) fandom.com PageRank:1.002580594391505\n",
    "10) tripadvisor.com PageRank:0.9650177901770668\n",
    "11) apple.com PageRank:0.889833856208652\n",
    "12) urbandictionary.com PageRank:0.783325788483356\n",
    "13) wiktionary.org PageRank:0.7344188304535614\n",
    "14) instagram.com PageRank:0.488708566583417\n",
    "15) blocket.se PageRank:0.4475515972949302\n",
    "16) yelp.com PageRank:0.40433532158209406\n",
    "17) hh.se PageRank:0.3986772304305001\n",
    "18) visithalland.com PageRank:0.2340398586683635\n",
    "19) halmstad.se PageRank:0.19753243545269022\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "__IMPORTANT__ \n",
    "\n",
    "Please complete this Jupyter Notebook file. Then __rename the file using following format__:  \"`BDPP_2023_Lab2_NameSurname.ipynb`\" and submit it to the BalckBoard __within the specified deadline in blackboard__. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
