{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDPP Course Lab 1 (Introduction and Installation)\n",
    "\n",
    "<img src='files/clouds.png'></img>\n",
    "\n",
    "Welcome to the first lab of the BDPP course!\n",
    "\n",
    "This notebook guides you through the basic concepts to start working with Spark. Here is a summary of what we will cover in this lab:\n",
    "\n",
    "1. About Spark\n",
    "1. Three methods of running Spark:\n",
    "    1. Use a local environment (__optional__)\n",
    "    1. Use a docker image (__optional__)\n",
    "    1. Use a cloud platform:\n",
    "        1. Google Cloud Platform \n",
    "        1. IBM Watson Studio \n",
    "        1. Amazon Web Services \n",
    "        \n",
    "__Note__: Running spark on one of the mentioned cloud platforms is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. About Spark\n",
    "Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for processing structured data, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n",
    "\n",
    "<img src='files/spark.png' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "\n",
    "A Spark program has a driver program and worker programs. Worker programs run on cluster nodes or in local threads. Data sets are distributed\u001d",
    " across workers. \n",
    "\n",
    "<img src='files/Spark Architecture.png' width=\"50%\" height=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sparkinstallation\"></a>\n",
    "## 2. Three methods of running Spark\n",
    "\n",
    "The first step is to get familiar with different ways of accessing the Spark programming environment. In this lab, we introduce a couple of methods to run Spark on your local machine as well as on the cloud. You may run your lab assignments on your local computer. Alternatively, The cloud platform can be utilized for the project or whenever you want to scale up your process. \n",
    "\n",
    "__Note:__ in case the specified software version is outdated; you can always use the latest version.\n",
    "\n",
    "<a id=\"sparkinstallation1\"></a>\n",
    "\n",
    "\n",
    "### 2.A. Set up a local environment\n",
    "\n",
    "This notebook uses pySpark, the Python API for Spark. You should follow the following steps to install PySpark locally:\n",
    "- Install Python\n",
    "- Install Java\n",
    "- Download Spark\n",
    "- Install pyspark\n",
    "\n",
    "#### Install Python\n",
    "\n",
    "We use **Python** as the programming language, **Anaconda** as the virtual environment and package manager, and finally **Jupyter notebook** as the development environment. If you haven't installed Python yet, we recommend you install it through [Anaconda](https://anaconda.org/). Note that this course is not intended to teach python programming. So, it is recommended to have some python programming background.\n",
    "\n",
    "Please follow these steps to prepare the development environment:\n",
    "- Download Anaconda (Python 3.7 version) from https://www.anaconda.com/distribution/, and install it on your machine.\n",
    "- Create an environment with Python 3.7 and use `pp_course` as the environment name. You can find information about creating and managing anaconda environments using navigator interface from https://docs.anaconda.com/anaconda/navigator/tutorials/manage-environments/#creating-a-new-environment. You can also do it through shell commands https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html. Here is a list of useful commands:\n",
    "    - `conda create -n pp_course python=3.7`\n",
    "    - `conda env list`\n",
    "    - `source activate pp_course` for unix/mac, and `conda activate pp_course` for windows\n",
    "- Jupyter editor should be installed on your environment by default. If it is not, use Anaconda navigator to install it. To do so, first, go to the Home tab, then select pp_course environment. Finally, from the list of applications, find Jupyter Notebook, and install it. \n",
    "- Now you should be able to launch Jupyter. This launches a new browser window (or a new tab) showing the notebook Dashboard, a sort of control panel that allows (among other things) to select which notebook to open. You can also launch Jupyter using shell command. To do so, first, activate `pp_course` environment, then run the following command: `jupyter notebook`\n",
    "- Play with the interface and try to figure out its functionalities. You can find lots of useful information about notebook from the following [link](https://buildmedia.readthedocs.org/media/pdf/jupyter-notebook/latest/jupyter-notebook.pdf).\n",
    "\n",
    "#### Install Java\n",
    "\n",
    "You need Java installed on your machine since Spark runs on top of JVM. Download and install Java JDK. You can download it from the following link:\n",
    "https://www.oracle.com/java/technologies/downloads/#jdk17-windows\n",
    "\n",
    "Add `JAVA_HOME` environment variable to your system (**Note:** This step depends on your operating system.):\n",
    "- on Unix/mac: `export JAVA_HOME=\"/path/to/the/java/folder\"`\n",
    "- on Windows see this [link](https://confluence.atlassian.com/doc/setting-the-java_home-variable-in-windows-8895.html)\n",
    "\n",
    "#### Download Spark\n",
    "\n",
    "Spark is an open-source project under Apache Software Foundation. Spark can be downloaded here:\n",
    "\n",
    "https://spark.apache.org/downloads.html\n",
    "\n",
    "First, choose a Spark release, e,g, 3.X.X, and choose pre-build for Apache Hadoop, e,g, 2.7. Next, click on the download link and download the file. We recommend moving the file to your home directory, uncompress it, and maybe rename it to a shorter name such as `spark`. Now the spark file should be located here.\n",
    "\n",
    "`/your/home/directory/spark`\n",
    "\n",
    "Add `SPARK_HOME` environment variable to your system (**Note:** This step depends on your operating system):\n",
    "\n",
    "- on Unix/mac, e.g.: `export SPARK_HOME=\"/your/home/directory/spark-3.1.1-bin-hadoop2.7\"` and then `export PATH=\"$SPARK_HOME/bin:$PATH\"`\n",
    "- on Windows: Similarly to the Java installation, set `SPARK_HOME` and add `%SPARK_HOME%\\bin` in PATH variable in System Variables.\n",
    "\n",
    "<a id=\"sparkinstallation2\"></a>\n",
    "\n",
    "Now, launch your terminal and run the command below to test the installation.\n",
    "\n",
    "`spark-shell`\n",
    "\n",
    "You should be able to see this:\n",
    "\n",
    "```\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.1.1\n",
    "      /_/         \n",
    "```\n",
    "#### Install pyspark\n",
    "\n",
    "The final step is to install pyspark. Run the following command to install pyspark. (if you are using python without Anaconda, use the following alternative command: `pip install pyspark`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
      "Collecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824036 sha256=af11db9d4d7ba02c9e0fecd32c34b3214a9b4f46b1efd60d480e99fcdc67918e\n",
      "  Stored in directory: c:\\users\\nkill\\appdata\\local\\pip\\cache\\wheels\\6c\\e3\\9b\\0525ce8a69478916513509d43693511463c6468db0de237c86\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
     ]
    }
   ],
   "source": [
    "# Installing pyspark package in the current Jupyter kernel\n",
    "import sys\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to __restart your notebook kernel__. You can directly access spark by running `pyspark` command in your console. In case you still have problem running spark on your local machine, you may find useful information in this [link](https://sigdelta.com/blog/how-to-install-pyspark-locally/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B. Use a docker image\n",
    "\n",
    "One of the easiest approaches for setting up a Spark environment is to use an available docker image. Setting up a Docker container on your local machine is pretty simple.  Download docker from the [docker website](https://www.docker.com/get-started) and run the following command in the terminal:\n",
    "\n",
    "`docker run -it -p 8888:8888 jupyter/pyspark-notebook`\n",
    "\n",
    "navigate to http://localhost:8888 in your browser, and you see the following screen:\n",
    "\n",
    "<img src=files/docker1.png width=\"500\">\n",
    "\n",
    "In your terminal, you should see a token:\n",
    "\n",
    "<img src=files/docker2.png width=\"500\">\n",
    "\n",
    "Copy and paste this token, the numbers following `/?token=`, into the token textbook. With that done, you are all set to go! Spark is already installed in the container. You are all ready to open up a notebook and start writing some Spark code. \n",
    "\n",
    "<a id=\"sparkinstallation3\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.C.a Use a cloud platform (Google Cloud Platform)\n",
    "\n",
    "The final approach that we introduce for accessing Spark is through a cloud platform. The cloud platform, along with Spark enable you to easily scale up your computation power according to your demand. We will start with Google Cloud Platform (GCP). For GCP, you need to have an account. \n",
    "\n",
    "\n",
    "We have uploaded a URL in Course Material menu in BlackBoeard that you will need to access in order to request a Google Cloud coupon. You will be asked to provide your school email address and name. An email will be sent to you to confirm these details before a coupon is sent to you.\n",
    "You can only request ONE code per unique email address.\n",
    "\n",
    "After you redeemed your Google ccloud coupon, follow the provided instructions to prepare your spark programming environment on the cloud:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a project\n",
    "<img src=\"files/GCP0.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "<img src=\"files/GCP1.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "<img src=\"files/GCP2.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<img src=\"files/GCP2_1.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a cluster\n",
    "<img src=\"files/GCP3.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<img src=\"files/GCP3_1.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "<img src=\"files/GCP4.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\n",
    "<img src=\"files/GCP5.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\n",
    "<img src=\"files/GCP6_1.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\n",
    "<img src=\"files/GCP7.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Create firewall rule\n",
    "<img src=\"files/GCP8.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\n",
    "<img src=\"files/GCP9.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\n",
    "<img src=\"files/GCP10.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\n",
    "<img src=\"files/GCP11_1.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.\n",
    "<img src=\"files/GCP12.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Set static IP\n",
    "<img src=\"files/GCP13.png\" alt=\"Drawing\"/>\n",
    "<img src=\"files/GCP13_1.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.\n",
    "<img src=\"files/GCP14.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Create a Bucket\n",
    "<img src=\"files/GCP15.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16.\n",
    "<img src=\"files/GCP16.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17.\n",
    "<img src=\"files/GCP17_2.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Uploading a CSV file\n",
    "<img src=\"files/GCP18.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19.\n",
    "<img src=\"files/GCP19_1.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. Configure and run Jupyter\n",
    "<img src=\"files/GCP20.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21.\n",
    "<img src=\"files/GCP21.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22.\n",
    "\n",
    "`jupyter notebook --generate-config`\n",
    "<img src=\"files/GCP22.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23.\n",
    "\n",
    "`vi ~/.jupyter/jupyter_notebook_config.py`\n",
    "<img src=\"files/GCP23.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24. Edit jupyter_notebook_config.py\n",
    "\n",
    "Press \"i\" and insert the following lines:\n",
    "\n",
    "- c = get_config()\n",
    "- c.NotebookApp.ip = '*'\n",
    "- c.NotebookApp.open_browser = False\n",
    "- c.NotebookApp.port = 8888\n",
    "\n",
    "Then Press Esc and \":wq\" to write the configuration file and exit.\n",
    "\n",
    "<img src=\"files/GCP24_1.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25. Run jupyter notebook\n",
    "<img src=\"files/GCP25.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26. Access the notebook from a local machine\n",
    "\n",
    "Now in a new tap in your local browser, open the following URL address:\n",
    "\n",
    "http://<your cluster external ip address (see image below on how to get this number)>:8888/\n",
    "\n",
    "e.g. http://35.232.73.232:8888/\n",
    "\n",
    "<img src=\"files/GCP26.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27. You should be able to see jupyer home page\n",
    "\n",
    "<img src=\"files/GCP27.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use commands from `read_csv_file_from_bucket.ipynb` to load your csv file. This file is included in a folder named `files`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember to shut down clusters\n",
    "\n",
    "Remember to shut down clusters when you are done working with them; otherwise, you will quickly lose your credit. Following shows the steps for stopping the clusters:\n",
    "\n",
    "\n",
    "<img src='files/shutdown_clusters0.png' width=\"800\">\n",
    "\n",
    "\n",
    "<img src='files/shutdown_clusters.png' width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.C.b. IBM Watson\n",
    "\n",
    "In this section, we use the IBM cloud as our platform and IBM Watson Studio as our developing environment. The first step is to go to https://www.ibm.com/cloud and create a free account.\n",
    "<img src='files/ibm cloud1.png' width=\"800\">\n",
    "\n",
    "Next, log in to your account and confirm the required Acknowledgements by IBM. Now, you should be able to see your dashboard. According to the image below, search for Watson studio and run it.\n",
    "\n",
    "<img src='files/search watson.png' width=\"800\">\n",
    "\n",
    "Next, select the Lite plan and create your kernel. Remember that you can have only one instance of a Lite plan per service. To create a new instance, you have to delete your existing Lite plan instance.\n",
    "\n",
    "<img src='files/select watson plan.png' width=\"800\">\n",
    "\n",
    "And get started working.\n",
    "\n",
    "<img src='files/watson start2.png' width=\"800\">\n",
    "\n",
    "Now you must be able to create a Watson Studio project and add an empty notebook to your project. This opens an empty jupyter notebook with SparkContext available for use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.C.c. Amazon Web Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in case you are interested to use Amazon Web Services (aws), you can find a step by step walkthrough on how to set up your cloud enviroment in the following link: https://www.youtube.com/watch?v=r-ig8zpP3EM. The process is very similar to GCP.\n",
    "\n",
    "<img src='files/aws1.png' width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "__IMPORTANT__ \n",
    "\n",
    "This lab contains no assignment and you __do not need to submit__ it to blackboard!\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
